import { describe, it, expect } from 'vitest';
import {
  buildEvidenceSummary,
  renderStatusBlock,
  renderValidationBlock,
  applyAutogenBlock,
  reconcileGates,
  reconcileStatusContents,
  reconcileImplementationStatusContents,
  type EvidenceManifestSummary,
} from '../evaluation/evidence_reconciliation.js';

const BASE_MANIFEST: EvidenceManifestSummary = {
  generatedAt: '2026-02-02T00:00:00.000Z',
  metrics: {
    retrievalRecallAt5: { mean: 0.82, target: 0.8, met: true },
    contextPrecision: { mean: 0.74, target: 0.7, met: true },
    hallucinationRate: { mean: 0.03, target: 0.05, met: true },
    faithfulness: { mean: 0.87, target: 0.85, met: true },
    answerRelevancy: { mean: 0.79, target: 0.75, met: true },
  },
  ab: {
    lift: 0.2083333333,
    pValue: 0.0937140553,
    targetLift: 0.2,
    significant: false,
    controlSampleSize: 80,
    treatmentSampleSize: 80,
    nPerArm: 80,
  },
  performance: {
    p50LatencyMs: 633,
    p99LatencyMs: 1260,
    targetP50LatencyMs: 500,
    targetP99LatencyMs: 1000,
    memoryPerKLOC: 222.78,
    targetMemoryPerKLOC: 50,
  },
  scenarios: {
    total: 30,
    passing: 30,
    failing: 0,
  },
};

describe('evidence reconciliation summaries', () => {
  it('builds a status block with explicit MET/NOT MET markers', () => {
    const summary = buildEvidenceSummary(BASE_MANIFEST);
    const block = renderStatusBlock(summary);
    expect(block).toContain('Retrieval Recall@5');
    expect(block).toContain('Memory per 1K LOC');
    expect(block).toContain('NOT MET');
    expect(block).toContain('p-value');
  });

  it('builds a validation block that reflects scenario coverage', () => {
    const summary = buildEvidenceSummary(BASE_MANIFEST);
    const block = renderValidationBlock(summary);
    expect(block).toContain('Scenario Families');
    expect(block).toContain('30/30');
  });

  it('replaces autogen blocks between markers', () => {
    const content = [
      'Header',
      '<!-- EVIDENCE_AUTOGEN_START -->',
      'old',
      '<!-- EVIDENCE_AUTOGEN_END -->',
      'Footer',
    ].join('\n');
    const updated = applyAutogenBlock(content, 'EVIDENCE_AUTOGEN', 'new block');
    expect(updated).toContain('new block');
    expect(updated).not.toContain('old');
  });

  it('reconciles status narrative without strict marker annotations', () => {
    const summary = buildEvidenceSummary(BASE_MANIFEST);
    const content = [
      'Header',
      'Last Verified: 2026-01-01',
      '<!-- EVIDENCE_AUTOGEN_START -->',
      'old',
      '<!-- EVIDENCE_AUTOGEN_END -->',
      '## ⚠️ PROJECT STATUS (2026-01-01)',
      '### Infrastructure: evidence-backed (see autogenerated block)',
      '- claim one',
      '| Col | Val |',
      '| --- | --- |',
      '| A | B |',
      '## How to Use This File',
      'Footer',
    ].join('\n');
    const updated = reconcileStatusContents(content, summary);
    expect(updated).toContain(renderStatusBlock(summary));
    expect(updated).toContain('Last Verified: 2026-02-02');
    expect(updated).toContain('- claim one');
    expect(updated).toContain('Infrastructure: evidence-backed (see autogenerated block)');
    expect(updated).not.toContain('unverified_by_trace(');
  });

  it('reconciles implementation status with evidence block and strips strict marker tokens', () => {
    const summary = buildEvidenceSummary(BASE_MANIFEST);
    const content = [
      '# Implementation Status',
      '> **Updated**: 2026-01-01',
      '## ⚠️ CURRENT STATUS: VALIDATION PENDING',
      '<!-- IMPLEMENTATION_STATUS_AUTOGEN_START -->',
      'old block',
      '<!-- IMPLEMENTATION_STATUS_AUTOGEN_END -->',
      '**Infrastructure (Phases 0-11)**: ✅ COMPLETE',
      '- claim one',
      '## How This Document Relates to the Librarian Story',
    ].join('\n');
    const updated = reconcileImplementationStatusContents(content, summary);
    expect(updated).toContain('Implementation Evidence Snapshot');
    expect(updated).toContain('Updated**: 2026-02-02');
    expect(updated).toContain('**Infrastructure (Phases 0-11)**: ✅ COMPLETE');
    expect(updated).toContain('- claim one');
    expect(updated).not.toContain('unverified_by_trace(');
  });

  it('updates gates validation status from summary', () => {
    const summary = buildEvidenceSummary(BASE_MANIFEST);
    const gates = {
      lastUpdated: 'old',
      description: 'All validation phases COMPLETE.',
      tasks: {
        'layer7.metricsRAGAS': { status: 'pass' },
        'layer7.abExperiments': { status: 'pass' },
        'layer7.scenarioFamilies': { status: 'pass' },
        'layer7.performanceBenchmark': { status: 'pass' },
        'layer7.unrelated': { status: 'pass' },
      },
      validationStatus: {
        blockingMetrics: {},
      },
    };
    const updated = reconcileGates(gates, summary, { evidencePaths: ['eval-results/metrics-report.json'] });
    expect(updated.lastUpdated).toBe(summary.generatedAt);
    expect(updated.validationStatus.blockingMetrics).toHaveProperty('Retrieval Recall@5');
    expect(updated.description).not.toContain('COMPLETE');
    expect(updated.tasks['layer7.metricsRAGAS'].status).toBe('pass');
    expect(updated.tasks['layer7.abExperiments'].status).toBe('fail');
    expect(updated.tasks['layer7.abExperiments'].note).toContain('p=0.094 > α=0.05 (not significant)');
    expect(updated.tasks['layer7.unrelated'].status).toBe('fail');
    expect(updated.tasks['layer7.unrelated'].note).toContain('missing_evidence_links');
    expect(updated.tasks['layer7.unrelated'].note).not.toContain('unverified_by_trace');
  });

  it('fails ab gate when n-per-arm is below required minimum', () => {
    const summary = buildEvidenceSummary({
      ...BASE_MANIFEST,
      ab: {
        ...BASE_MANIFEST.ab,
        lift: 0.31,
        pValue: 0.01,
        significant: true,
        controlSampleSize: 12,
        treatmentSampleSize: 14,
        nPerArm: 12,
      },
    });
    const gates = {
      tasks: {
        'layer7.abExperiments': { status: 'pass' },
      },
      validationStatus: {
        blockingMetrics: {},
      },
    };

    const updated = reconcileGates(gates, summary, { evidencePaths: ['eval-results/ab-results.json'] });
    expect(updated.tasks['layer7.abExperiments'].status).toBe('fail');
    expect(updated.tasks['layer7.abExperiments'].note).toContain('n_per_arm=12 < min 30');
  });

  it('passes ab gate when lift, p-value, and n-per-arm all meet thresholds', () => {
    const summary = buildEvidenceSummary({
      ...BASE_MANIFEST,
      ab: {
        ...BASE_MANIFEST.ab,
        lift: 0.31,
        pValue: 0.01,
        significant: true,
        controlSampleSize: 36,
        treatmentSampleSize: 34,
        nPerArm: 34,
      },
    });
    const gates = {
      tasks: {
        'layer7.abExperiments': { status: 'fail' },
      },
      validationStatus: {
        blockingMetrics: {},
      },
    };

    const updated = reconcileGates(gates, summary, { evidencePaths: ['eval-results/ab-results.json'] });
    expect(updated.tasks['layer7.abExperiments'].status).toBe('pass');
  });

  it('writes explicit ab gate thresholds into gate definitions', () => {
    const summary = buildEvidenceSummary(BASE_MANIFEST);
    const gates = {
      tasks: {
        'layer5.abExperiments': { status: 'not_started' },
        'layer7.abExperiments': { status: 'fail' },
      },
      validationStatus: {
        blockingMetrics: {},
      },
    };

    const updated = reconcileGates(gates, summary, { evidencePaths: ['eval-results/ab-results.json'] });
    expect(updated.tasks['layer5.abExperiments'].p_threshold).toBe(0.05);
    expect(updated.tasks['layer5.abExperiments'].min_sessions_per_arm).toBe(30);
    expect(updated.tasks['layer7.abExperiments'].p_threshold).toBe(0.05);
    expect(updated.tasks['layer7.abExperiments'].min_sessions_per_arm).toBe(30);
  });
});
