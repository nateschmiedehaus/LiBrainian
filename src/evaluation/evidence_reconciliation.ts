type MetricSummary = {
  mean: number;
  target: number;
  met: boolean;
};

export type EvidenceManifestSummary = {
  generatedAt: string;
  metrics: {
    retrievalRecallAt5: MetricSummary;
    contextPrecision: MetricSummary;
    hallucinationRate: MetricSummary;
    faithfulness: MetricSummary;
    answerRelevancy: MetricSummary;
  };
  ab: {
    lift: number;
    pValue: number;
    targetLift: number;
    significant: boolean;
  };
  performance: {
    p50LatencyMs: number;
    p99LatencyMs: number;
    targetP50LatencyMs: number;
    targetP99LatencyMs: number;
    memoryPerKLOC: number;
    targetMemoryPerKLOC: number;
    latencyByQueryType?: Record<string, unknown>;
    latencyQueryCounts?: Record<string, unknown>;
  };
  scenarios: {
    total: number;
    passing: number;
    failing: number;
  };
};

export type EvidenceSummary = EvidenceManifestSummary & {
  derived: {
    memoryMet: boolean;
    latencyMet: boolean;
    liftMet: boolean;
    scenarioPassRate: number;
  };
};

export type GateReconcileOptions = {
  evidencePaths?: string[];
  unverifiedReason?: string;
};

type GateTask = {
  status?: string;
  lastRun?: string;
  measured?: unknown;
  note?: string;
  evidence?: unknown;
  verified?: boolean;
  layer?: number;
  [key: string]: unknown;
};

type GateValidationStatus = {
  blockingMetrics?: Record<string, { target: string; measured: string; status: string }>;
  lastVerified?: string;
  totalWorkUnits?: unknown;
  completedWorkUnits?: unknown;
  infrastructure?: string;
  validation?: string;
  note?: string;
  [key: string]: unknown;
};

type GateSet = {
  lastUpdated?: string;
  description?: string;
  validationStatus?: GateValidationStatus;
  tasks?: Record<string, GateTask>;
  summary?: Record<string, Record<string, number>>;
  [key: string]: unknown;
};

const DEFAULT_UNVERIFIED_REASON = 'evidence_manifest_missing';

function round(value: number, digits = 2): number {
  const factor = 10 ** digits;
  return Math.round(value * factor) / factor;
}

function formatDate(value: string): string {
  return value.split('T')[0] ?? value;
}

function normalizeStrictMarkerTokens(text: string): string {
  return text
    .replace(/unverified_by_trace\(([^)]+)\):\s*/g, 'unverified ($1): ')
    .replace(/unverified_by_trace\(([^)]+)\)/g, 'unverified ($1)');
}

function normalizeStrictMarkersInValue<T>(value: T): T {
  if (typeof value === 'string') {
    return normalizeStrictMarkerTokens(value) as T;
  }
  if (Array.isArray(value)) {
    return value.map((entry) => normalizeStrictMarkersInValue(entry)) as T;
  }
  if (value && typeof value === 'object') {
    const output: Record<string, unknown> = {};
    for (const [key, entry] of Object.entries(value as Record<string, unknown>)) {
      output[key] = normalizeStrictMarkersInValue(entry);
    }
    return output as T;
  }
  return value;
}

export function buildEvidenceSummary(manifest: EvidenceManifestSummary): EvidenceSummary {
  const memoryMet = manifest.performance.memoryPerKLOC <= manifest.performance.targetMemoryPerKLOC;
  const latencyMet = (
    manifest.performance.p50LatencyMs <= manifest.performance.targetP50LatencyMs
    && manifest.performance.p99LatencyMs <= manifest.performance.targetP99LatencyMs
  );
  const liftMet = manifest.ab.lift >= manifest.ab.targetLift;
  const scenarioPassRate = manifest.scenarios.total > 0
    ? manifest.scenarios.passing / manifest.scenarios.total
    : 0;
  return {
    ...manifest,
    derived: {
      memoryMet,
      latencyMet,
      liftMet,
      scenarioPassRate,
    },
  };
}

export function renderStatusBlock(summary: EvidenceSummary): string {
  const metrics = summary.metrics;
  const rows = [
    ['Retrieval Recall@5', metrics.retrievalRecallAt5.target, metrics.retrievalRecallAt5.mean, metrics.retrievalRecallAt5.met],
    ['Context Precision', metrics.contextPrecision.target, metrics.contextPrecision.mean, metrics.contextPrecision.met],
    ['Hallucination Rate', metrics.hallucinationRate.target, metrics.hallucinationRate.mean, metrics.hallucinationRate.met],
    ['Faithfulness', metrics.faithfulness.target, metrics.faithfulness.mean, metrics.faithfulness.met],
    ['Answer Relevancy', metrics.answerRelevancy.target, metrics.answerRelevancy.mean, metrics.answerRelevancy.met],
  ];

  const metricLines = rows
    .map(([name, target, measured, met]) => {
      const status = met ? 'MET' : 'NOT MET';
      return `| ${name} | ${target} | ${measured} | ${status} |`;
    })
    .join('\n');

  const memoryStatus = summary.derived.memoryMet ? 'MET' : 'NOT MET';
  const latencyStatus = summary.derived.latencyMet ? 'MET' : 'NOT MET';
  const liftStatus = summary.derived.liftMet ? 'MET' : 'NOT MET';
  const scenarioRate = round(summary.derived.scenarioPassRate * 100, 1);

  return [
    '### Evidence-Backed Status (Autogenerated)',
    `Generated: ${summary.generatedAt}`,
    '',
    '| Metric | Target | Measured | Status |',
    '| --- | --- | --- | --- |',
    metricLines,
    '',
    `| A/B Lift | ${summary.ab.targetLift} | ${round(summary.ab.lift, 4)} (p-value ${round(summary.ab.pValue, 4)}) | ${liftStatus} |`,
    `| Query Latency p50/p99 | <= ${summary.performance.targetP50LatencyMs}/${summary.performance.targetP99LatencyMs} ms | ${round(summary.performance.p50LatencyMs, 1)}/${round(summary.performance.p99LatencyMs, 1)} ms | ${latencyStatus} |`,
    `| Memory per 1K LOC | ${summary.performance.targetMemoryPerKLOC} MB | ${round(summary.performance.memoryPerKLOC, 2)} MB | ${memoryStatus} |`,
    '',
    `Scenario Families: ${summary.scenarios.passing}/${summary.scenarios.total} (${scenarioRate}%)`,
  ].join('\n');
}

export function renderValidationBlock(summary: EvidenceSummary): string {
  const scenarioRate = round(summary.derived.scenarioPassRate * 100, 1);
  return [
    '### Validation Summary (Autogenerated)',
    `Generated: ${summary.generatedAt}`,
    '',
    `Scenario Families: ${summary.scenarios.passing}/${summary.scenarios.total} (${scenarioRate}%)`,
    `A/B Lift: ${round(summary.ab.lift, 4)} (p-value ${round(summary.ab.pValue, 4)})`,
    `Memory per 1K LOC: ${round(summary.performance.memoryPerKLOC, 2)} MB (target ${summary.performance.targetMemoryPerKLOC} MB)`,
  ].join('\n');
}

export function renderImplementationStatusBlock(summary: EvidenceSummary): string {
  const scenarioRate = round(summary.derived.scenarioPassRate * 100, 1);
  const liftStatus = summary.derived.liftMet ? 'MET' : 'NOT MET';
  const memoryStatus = summary.derived.memoryMet ? 'MET' : 'NOT MET';
  const latencyStatus = summary.derived.latencyMet ? 'MET' : 'NOT MET';
  return [
    '### Implementation Evidence Snapshot (Autogenerated)',
    `Generated: ${summary.generatedAt}`,
    '',
    `Retrieval Recall@5: ${round(summary.metrics.retrievalRecallAt5.mean, 2)} (target ${summary.metrics.retrievalRecallAt5.target})`,
    `Context Precision: ${round(summary.metrics.contextPrecision.mean, 2)} (target ${summary.metrics.contextPrecision.target})`,
    `Hallucination Rate: ${round(summary.metrics.hallucinationRate.mean, 3)} (target ${summary.metrics.hallucinationRate.target})`,
    `Faithfulness: ${round(summary.metrics.faithfulness.mean, 2)} (target ${summary.metrics.faithfulness.target})`,
    `Answer Relevancy: ${round(summary.metrics.answerRelevancy.mean, 2)} (target ${summary.metrics.answerRelevancy.target})`,
    '',
    `A/B Lift: ${round(summary.ab.lift, 4)} (target ${summary.ab.targetLift}, p-value ${round(summary.ab.pValue, 4)}) → ${liftStatus}`,
    `Query Latency p50/p99: ${round(summary.performance.p50LatencyMs, 1)}ms/${round(summary.performance.p99LatencyMs, 1)}ms `
    + `(targets ${summary.performance.targetP50LatencyMs}/${summary.performance.targetP99LatencyMs}ms) → ${latencyStatus}`,
    `Memory per 1K LOC: ${round(summary.performance.memoryPerKLOC, 2)} MB (target ${summary.performance.targetMemoryPerKLOC} MB) → ${memoryStatus}`,
    `Scenario Families: ${summary.scenarios.passing}/${summary.scenarios.total} (${scenarioRate}%)`,
    '',
    'Narrative status below has not been reconciled to evidence.',
  ].join('\n');
}

export function applyAutogenBlock(contents: string, markerName: string, newBlock: string): string {
  const start = `<!-- ${markerName}_START -->`;
  const end = `<!-- ${markerName}_END -->`;
  const startIdx = contents.indexOf(start);
  const endIdx = contents.indexOf(end);

  if (startIdx === -1 || endIdx === -1 || endIdx < startIdx) {
    return `${contents}\n${start}\n${newBlock}\n${end}`;
  }

  const before = contents.slice(0, startIdx + start.length);
  const after = contents.slice(endIdx);
  return `${before}\n${newBlock}\n${after}`;
}

export function reconcileGates(
  gates: GateSet,
  summary: EvidenceSummary,
  options: GateReconcileOptions = {},
): GateSet {
  const next = JSON.parse(JSON.stringify(gates ?? {})) as GateSet;
  const reason = options.unverifiedReason ?? DEFAULT_UNVERIFIED_REASON;
  const evidencePaths = new Set(options.evidencePaths ?? []);
  next.lastUpdated = summary.generatedAt;
  if (typeof next.description === 'string') {
    next.description = 'Machine-readable gate status with tiered testing architecture (evidence reconciled).';
  }
  const summaryDate = formatDate(summary.generatedAt);
  const derivedMetricsMet = Object.values(summary.metrics).every((metric) => metric.met);
  const blockingMetrics: Record<string, { target: string; measured: string; status: string }> = {
    'Retrieval Recall@5': {
      target: `>=${summary.metrics.retrievalRecallAt5.target}`,
      measured: `${summary.metrics.retrievalRecallAt5.mean}`,
      status: summary.metrics.retrievalRecallAt5.met ? 'MET' : 'NOT MET',
    },
    'Context Precision': {
      target: `>=${summary.metrics.contextPrecision.target}`,
      measured: `${summary.metrics.contextPrecision.mean}`,
      status: summary.metrics.contextPrecision.met ? 'MET' : 'NOT MET',
    },
    'Hallucination Rate': {
      target: `<${summary.metrics.hallucinationRate.target}`,
      measured: `${summary.metrics.hallucinationRate.mean}`,
      status: summary.metrics.hallucinationRate.met ? 'MET' : 'NOT MET',
    },
    'Faithfulness': {
      target: `>=${summary.metrics.faithfulness.target}`,
      measured: `${summary.metrics.faithfulness.mean}`,
      status: summary.metrics.faithfulness.met ? 'MET' : 'NOT MET',
    },
    'Answer Relevancy': {
      target: `>=${summary.metrics.answerRelevancy.target}`,
      measured: `${summary.metrics.answerRelevancy.mean}`,
      status: summary.metrics.answerRelevancy.met ? 'MET' : 'NOT MET',
    },
    'A/B Lift': {
      target: `>=${summary.ab.targetLift}`,
      measured: `${summary.ab.lift} (p=${summary.ab.pValue})`,
      status: summary.derived.liftMet ? 'MET' : 'NOT MET',
    },
    'Memory per 1K LOC': {
      target: `<=${summary.performance.targetMemoryPerKLOC} MB`,
      measured: `${summary.performance.memoryPerKLOC} MB`,
      status: summary.derived.memoryMet ? 'MET' : 'NOT MET',
    },
  };

  if (!next.validationStatus) {
    next.validationStatus = {};
  }
  next.validationStatus.blockingMetrics = blockingMetrics;
  next.validationStatus.lastVerified = summary.generatedAt;
  if ('totalWorkUnits' in next.validationStatus) {
    next.validationStatus.totalWorkUnits = null;
  }
  if ('completedWorkUnits' in next.validationStatus) {
    next.validationStatus.completedWorkUnits = null;
  }
  if ('infrastructure' in next.validationStatus) {
    next.validationStatus.infrastructure = `unverified (${reason})`;
  }
  if ('validation' in next.validationStatus) {
    next.validationStatus.validation = `unverified (${reason})`;
  }
  next.validationStatus.note = 'Non-manifest validation claims removed; only manifest metrics retained.';

  const tasks = (next.tasks ?? {}) as Record<string, GateTask>;
  const updateTask = (key: string, updates: Partial<GateTask>) => {
    if (!tasks[key]) return;
    tasks[key] = { ...tasks[key], ...updates };
  };

  updateTask('layer7.metricsRAGAS', {
    status: derivedMetricsMet ? 'pass' : 'fail',
    lastRun: summaryDate,
    measured: {
      recallAt5: summary.metrics.retrievalRecallAt5.mean,
      contextPrecision: summary.metrics.contextPrecision.mean,
      hallucinationRate: summary.metrics.hallucinationRate.mean,
      faithfulness: summary.metrics.faithfulness.mean,
      answerRelevancy: summary.metrics.answerRelevancy.mean,
    },
  });

  updateTask('layer5.retrievalRecall', {
    status: summary.metrics.retrievalRecallAt5.met ? 'pass' : 'fail',
    lastRun: summaryDate,
    measured: summary.metrics.retrievalRecallAt5.mean,
  });
  updateTask('layer5.retrievalPrecision', {
    status: summary.metrics.contextPrecision.met ? 'pass' : 'fail',
    lastRun: summaryDate,
    measured: summary.metrics.contextPrecision.mean,
  });
  updateTask('layer5.hallucinationRate', {
    status: summary.metrics.hallucinationRate.met ? 'pass' : 'fail',
    lastRun: summaryDate,
    measured: summary.metrics.hallucinationRate.mean,
  });

  updateTask('layer7.abExperiments', {
    status: summary.derived.liftMet && summary.ab.significant ? 'pass' : 'fail',
    lastRun: summaryDate,
    measured: `lift=${summary.ab.lift} (p=${summary.ab.pValue})`,
  });

  updateTask('layer7.scenarioFamilies', {
    status: summary.scenarios.total > 0 && summary.scenarios.passing === summary.scenarios.total ? 'pass' : 'fail',
    lastRun: summaryDate,
    measured: `${summary.scenarios.passing}/${summary.scenarios.total}`,
  });

  updateTask('layer7.performanceBenchmark', {
    status: summary.derived.memoryMet && summary.derived.latencyMet
      ? 'pass'
      : summary.derived.memoryMet || summary.derived.latencyMet
        ? 'partial'
        : 'fail',
    lastRun: summaryDate,
    measured: {
      p50LatencyMs: summary.performance.p50LatencyMs,
      p99LatencyMs: summary.performance.p99LatencyMs,
      memoryPerKLOC: summary.performance.memoryPerKLOC,
      targetP50LatencyMs: summary.performance.targetP50LatencyMs,
      targetP99LatencyMs: summary.performance.targetP99LatencyMs,
      latencyByQueryType: summary.performance.latencyByQueryType ?? {},
      latencyQueryCounts: summary.performance.latencyQueryCounts ?? {},
    },
    note: 'Measured real end-to-end query latency from query entry to response return; includes per-query-type breakdown.',
  });

  for (const [key, task] of Object.entries(tasks)) {
    if (
      [
        'layer7.metricsRAGAS',
        'layer5.retrievalRecall',
        'layer5.retrievalPrecision',
        'layer5.hallucinationRate',
        'layer7.abExperiments',
        'layer7.scenarioFamilies',
        'layer7.performanceBenchmark',
      ].includes(key)
    ) {
      continue;
    }

    const taskEvidence = JSON.stringify(task?.evidence ?? '');
    const taskMeasured = JSON.stringify(task?.measured ?? '');
    const taskNote = JSON.stringify(task?.note ?? '');
    const hasEvidence = Array.from(evidencePaths).some((path) =>
      `${taskEvidence} ${taskMeasured} ${taskNote}`.includes(path)
    );

    if (!hasEvidence && ['pass', 'implemented', 'complete'].includes(task?.status ?? '')) {
      tasks[key] = {
        ...task,
        status: 'unverified',
        note: 'Status not backed by evidence manifest.',
        verified: false,
      };
    }
  }

  next.tasks = tasks;
  const summaryCounts: Record<string, Record<string, number>> = {};
  for (const task of Object.values(tasks)) {
    const layerId = typeof task?.layer === 'number' ? `layer${task.layer}` : null;
    if (!layerId) continue;
    if (!summaryCounts[layerId]) {
      summaryCounts[layerId] = { total: 0 };
    }
    summaryCounts[layerId].total += 1;
    const status = typeof task.status === 'string' ? task.status : 'unknown';
    summaryCounts[layerId][status] = (summaryCounts[layerId][status] ?? 0) + 1;
  }
  next.summary = summaryCounts;
  return normalizeStrictMarkersInValue(next);
}

export function reconcileStatusContents(
  contents: string,
  summary: EvidenceSummary,
  options: { unverifiedReason?: string } = {},
): string {
  void options.unverifiedReason;
  let updated = applyAutogenBlock(contents, 'EVIDENCE_AUTOGEN', renderStatusBlock(summary));
  updated = updated.replace(/^Last Verified: .*$/m, `Last Verified: ${formatDate(summary.generatedAt)}`);
  return normalizeStrictMarkerTokens(updated);
}

export function reconcileImplementationStatusContents(
  contents: string,
  summary: EvidenceSummary,
  options: { unverifiedReason?: string } = {},
): string {
  void options.unverifiedReason;
  let updated = contents.replace(
    /^> \*\*Updated\*\*: .*$/m,
    `> **Updated**: ${formatDate(summary.generatedAt)}`,
  );
  updated = applyAutogenBlock(
    updated,
    'IMPLEMENTATION_STATUS_AUTOGEN',
    renderImplementationStatusBlock(summary),
  );
  return normalizeStrictMarkerTokens(updated);
}
