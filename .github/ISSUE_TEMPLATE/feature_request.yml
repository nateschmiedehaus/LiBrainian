name: Feature Request
description: Propose an improvement for LiBrainian UX or capabilities
title: "[FEATURE] "
labels:
  - enhancement
body:
  - type: markdown
    attributes:
      value: |
        Please describe the user problem first, then the proposed solution.
  - type: textarea
    id: problem
    attributes:
      label: Problem statement
      description: What user pain does this solve?
      placeholder: "New users struggle to understand whether quickstart completed correctly."
    validations:
      required: true
  - type: textarea
    id: impact
    attributes:
      label: Impact
      description: Who is affected and what workflow cost does this create?
      placeholder: "Agents lose time during onboarding and cannot reliably confirm setup health."
    validations:
      required: true
  - type: dropdown
    id: severity
    attributes:
      label: Severity
      description: Prioritized urgency for this feature.
      options:
        - critical
        - high
        - medium
        - low
    validations:
      required: true
  - type: dropdown
    id: area
    attributes:
      label: Area
      description: Primary product area.
      options:
        - cli
        - mcp
        - indexing
        - retrieval
        - bootstrap
        - docs
        - ci-release
        - evaluation
        - security
        - process
    validations:
      required: true
  - type: dropdown
    id: evidence_needed
    attributes:
      label: Evidence needed
      description: Evidence expected before we treat this as implementation-ready.
      options:
        - repro-required
        - traces-required
        - benchmark-required
    validations:
      required: true
  - type: dropdown
    id: user_impact
    attributes:
      label: User impact
      description: Primary user impact category.
      options:
        - blocker
        - degraded
        - minor
        - request
    validations:
      required: true
  - type: checkboxes
    id: user_journey
    attributes:
      label: User journey area
      options:
        - label: GitHub landing and discoverability
        - label: Installation / quickstart
        - label: Query quality and usefulness
        - label: Editing / contributor workflow
        - label: CI / publish / release readiness
  - type: textarea
    id: proposal
    attributes:
      label: Proposed solution
      placeholder: "Add a post-quickstart verification command with explicit pass/fail output."
    validations:
      required: true
  - type: textarea
    id: alternatives
    attributes:
      label: Alternatives considered
      placeholder: "Considered adding docs only, but command-level feedback is more reliable."
  - type: textarea
    id: repro_evidence
    attributes:
      label: Repro evidence
      description: Include concrete evidence (current command output, logs, screenshots, metrics) showing the gap.
      render: text
    validations:
      required: true
  - type: textarea
    id: baseline_runtime_evidence
    attributes:
      label: Baseline runtime evidence
      description: Link patrol artifacts/transcript excerpts and metrics that show present runtime behavior.
      placeholder: |
        - Artifact: state/patrol/patrol-run-...
        - Transcript: state/patrol/transcripts/...
        - Runtime observations:
    validations:
      required: true
  - type: textarea
    id: runtime_probe_success_criteria
    attributes:
      label: Runtime probe success criteria
      description: Define measurable runtime probes this feature must pass.
      placeholder: |
        - `npm run validate:reality` passes
        - Patrol comparison shows reduced critical findings for the target class
        - No contradictions across status/doctor/provider surfaces
    validations:
      required: true
  - type: input
    id: post_fix_verification_artifact
    attributes:
      label: Post-fix verification artifact
      description: Provide expected post-fix artifact path for validation and closure.
      placeholder: state/patrol/patrol-run-<label>-post-feature.json
    validations:
      required: true
  - type: textarea
    id: acceptance
    attributes:
      label: Acceptance criteria
      description: Define measurable completion conditions.
      placeholder: |
        - New users complete install-to-first-query in under 2 minutes.
        - docs/START_HERE.md includes exact verification command.
        - CLI output shows explicit success signal.
    validations:
      required: true
  - type: textarea
    id: affected_files
    attributes:
      label: affected-files
      description: List concrete files expected to change. Use one path per line.
      placeholder: |
        src/constructions/processes/patrol_process.ts
        src/cli/commands/status.ts
    validations:
      required: true
  - type: dropdown
    id: complexity_estimate
    attributes:
      label: complexity-estimate
      description: Agent complexity estimate for implementation.
      options:
        - S
        - M
        - L
    validations:
      required: true
  - type: input
    id: verification_command
    attributes:
      label: verification-command
      description: Exact command agents should run to verify implementation.
      placeholder: npm run build && npm test -- --run src/constructions/processes/__tests__/unit_patrol.test.ts
    validations:
      required: true
  - type: dropdown
    id: actionability
    attributes:
      label: Agent actionability label
      description: Choose which label triage should apply.
      options:
        - agent/actionable
        - agent/needs-human
    validations:
      required: true
  - type: dropdown
    id: contribution
    attributes:
      label: Will you contribute?
      options:
        - Yes, implementation
        - Yes, testing/docs
        - No, request only
    validations:
      required: true
  - type: markdown
    attributes:
      value: |
        ## Quality Analysis (filled by implementing agent)
        After implementing the feature, run `node scripts/issue-quality-analysis.mjs {issue_number} --description "..."` and fill in:
        ```
        - [ ] Ran `node scripts/issue-quality-analysis.mjs {issue_number}`
        - [ ] Read actual query results and assessed relevance
        - [ ] Agent assessment: [improved/no_change/degraded]
        - [ ] Concerns: [list any]
        ```
        The agent assessment is the primary quality evidence for retrieval/query/embedding/scoring/indexing changes.

        ## Closure Criteria (M0 issues)
        Before closing, paste this checklist in the closing comment:
        ```
        - [ ] Code merged to main: <PR link>
        - [ ] T0 passes: <CI link or npm test output>
        - [ ] T0.5 passes: <evidence>
        - [ ] Reality evidence: <type (a/b/c)> â€” <artifact or paste>
        ```
        See [REALITY_VERIFICATION.md](../../docs/LiBrainian/REALITY_VERIFICATION.md) for what counts as valid evidence.
